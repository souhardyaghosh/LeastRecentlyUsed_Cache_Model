# -*- coding: utf-8 -*-
"""cao_da.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18bUaE_AeIJHTOqkO1VmzZXd9ABHFwrLI
"""

!pip install pandas scikit-learn

"""DOING FROM THE START

Creating the LRU Cache with 20 elements and printing the cache with timestamps in tabular format
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Initialize cache with 20 elements
cache_size = 20
cache = [f'item_{i+1}' for i in range(cache_size)]

# Generate timestamps
base_time = datetime.now()
timestamps = [base_time - timedelta(minutes=i*5) for i in range(cache_size)]

# Create DataFrame
cache_df = pd.DataFrame({
    'Cache Element': cache,
    'Timestamp': timestamps
})

# Print cache elements with timestamps
print("Initial Cache Data:")
print(cache_df.to_string(index=False))

"""Randomizing the access to the cache and adding frequency of access"""

import random

# Simulate random access to cache
num_accesses = 100
access_data = {
    'Cache Element': [random.choice(cache) for _ in range(num_accesses)],
    'Access Timestamp': [base_time + timedelta(seconds=random.randint(1, 3600)) for _ in range(num_accesses)]
}

# Convert to DataFrame
access_df = pd.DataFrame(access_data)

# Calculate frequency of access
frequency_df = access_df['Cache Element'].value_counts().reset_index()
frequency_df.columns = ['Cache Element', 'Frequency']

# Merge with initial cache DataFrame to include frequencies
cache_with_freq_df = pd.merge(cache_df, frequency_df, on='Cache Element', how='left')
cache_with_freq_df['Frequency'].fillna(0, inplace=True)

# Print cache elements with access frequency
print("\nCache Data with Access Frequency:")
print(cache_with_freq_df.to_string(index=False))

"""Train the model with 1000 datasets

"""

import numpy as np
import pandas as pd

# Set random seed for reproducibility
np.random.seed(42)

# Parameters for the dataset
num_samples = 1000000
cache_elements = [f'item_{i+1}' for i in range(20)]  # 20 cache items

# Feature 1: Timestamp difference (simulated time intervals between accesses)
timestamp_diff = np.random.rand(num_samples)

# Feature 2: Access frequency (random integer frequency between 1 and 10)
access_freq = np.random.randint(1, 11, num_samples)

# Feature 3: Recency (simulated time since last access)
recency = np.random.rand(num_samples)

# Target: Cache item accessed
y = np.random.choice(cache_elements, num_samples)

# Create DataFrame
data = pd.DataFrame({
    'Timestamp_Difference': timestamp_diff,
    'Access_Frequency': access_freq,
    'Recency': recency,
    'Cache_Item': y
})

# Save dataset to CSV file
data.to_csv('cache_dataset.csv', index=False)
print("Dataset saved to 'cache_dataset.csv'.")

import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Set random seed for reproducibility KNN
np.random.seed(42)

# Generate synthetic data with more realistic features
num_samples = 1000000
num_features = 3  # [timestamp difference, access frequency, recency]

# Define the cache elements (e.g., 20 cache items)
cache_elements = [f'item_{i+1}' for i in range(20)]

# Feature 1: Randomly generated timestamp differences (time intervals between accesses)
timestamp_diff = np.random.rand(num_samples)

# Feature 2: Access frequency (random integer frequency between 1 and 10)
access_freq = np.random.randint(1, 11, num_samples)

# Feature 3: Recency (randomly generated "time since last access")
recency = np.random.rand(num_samples)

# Combine features into the feature matrix X
X = np.column_stack((timestamp_diff, access_freq, recency))

# Target: Randomly assign cache elements as the cache items accessed
y = np.random.choice(cache_elements, num_samples)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Hyperparameter tuning: Use GridSearchCV to find the best K, distance metric, and weights
param_grid = {
    'n_neighbors': [3, 5, 7, 10],   # Test different values for K
    'weights': ['uniform', 'distance'],  # Try uniform or distance-based weighting
    'p': [1, 2]  # p=1 for Manhattan, p=2 for Euclidean distance
}

knn = KNeighborsClassifier(n_jobs=-1)
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# Best parameters from grid search
best_params = grid_search.best_params_
print(f"Best Params: {best_params}")

# Train the KNN model using the best parameters
knn_best = KNeighborsClassifier(**best_params, n_jobs=-1)
knn_best.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = knn_best.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"KNN Model Accuracy after tuning: {accuracy:.2f}")

# Cross-validation to evaluate performance
cv_scores = cross_val_score(knn_best, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"Cross-Validation Accuracy: {np.mean(cv_scores):.2f}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Train Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)  # Using 100 trees in the forest
rf.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred_rf = rf.predict(X_test_scaled)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Model Accuracy: {accuracy_rf:.2f}")

# Cross-validation to evaluate performance
cv_scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)
print(f"Cross-Validation Accuracy (Random Forest): {np.mean(cv_scores_rf):.2f}")

"""Add 5 new elements and track eviction"""

# Define 5 new cache elements
new_cache_elements = [f'new_item_{i+1}' for i in range(5)]

# Simulate accessing these new cache elements
new_access_data = {
    'Cache Element': new_cache_elements,
    'Access Timestamp': [base_time + timedelta(seconds=random.randint(1, 3600)) for _ in range(5)],
    'Frequency': [1] * 5
}
new_access_df = pd.DataFrame(new_access_data)

# Print the new cache elements
print("\nNew Cache Elements:")
print(new_access_df.to_string(index=False))

# Simulate which items might be evicted
# Example: If we only keep the 20 most recent items, evict based on some logic
# Here we just print the new items and assume all are new
print("\nCache elements that might be evicted (based on KNN predictions):")
evicted_items = random.sample(cache, 5)  # Simulating eviction of 5 random items
print(evicted_items)

import numpy as np
import pandas as pd

# Set random seed for reproducibility
np.random.seed(42)

# Parameters for the dataset
num_samples = 1000000
cache_elements = [f'item_{i+1}' for i in range(20)]  # 20 cache items

# Feature 1: Timestamp difference (simulated time intervals between accesses)
timestamp_diff = np.random.rand(num_samples)

# Feature 2: Access frequency (random integer frequency between 1 and 10)
access_freq = np.random.randint(1, 11, num_samples)

# Feature 3: Recency (simulated time since last access)
recency = np.random.rand(num_samples)

# Target: Cache item accessed
y = np.random.choice(cache_elements, num_samples)

# Create DataFrame
data = pd.DataFrame({
    'Timestamp_Difference': timestamp_diff,
    'Access_Frequency': access_freq,
    'Recency': recency,
    'Cache_Item': y
})

# Save dataset to CSV file
data.to_csv('cache_dataset.csv', index=False)
print("Dataset saved to 'cache_dataset.csv'.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv('cache_dataset.csv')

# Prepare features and target
X = data[['Timestamp_Difference', 'Access_Frequency', 'Recency']].values
y = data['Cache_Item'].values

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Train Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)  # Using 100 trees in the forest
rf.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred_rf = rf.predict(X_test_scaled)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Model Accuracy: {accuracy_rf:.2f}")

# Cross-validation to evaluate performance
cv_scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)
print(f"Cross-Validation Accuracy (Random Forest): {np.mean(cv_scores_rf):.2f}")